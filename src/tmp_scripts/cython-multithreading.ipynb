{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "from random import randint\n",
    "from data_load import *\n",
    "in_dir= \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN\"\n",
    "a=MentionData('/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_x_new.txt',\n",
    "              \"/Users/mayk/working/figer/baseline/PLE/Intermediate/BBN/train_y.txt\",\n",
    "             in_dir+\"/feature.txt\",in_dir+\"/type.txt\")\n",
    "%load_ext Cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "\n",
    "cimport numpy as np\n",
    "from random import randint\n",
    "import sys\n",
    "import cython\n",
    "cdef extern from \"math.h\":\n",
    "    double sqrt(double m)\n",
    "import math\n",
    "from libc.stdlib cimport malloc, free\n",
    "\n",
    "from libc.math cimport exp\n",
    "from libc.math cimport log\n",
    "\n",
    "from libc.string cimport memset\n",
    "import random\n",
    "# scipy <= 0.15\n",
    "\n",
    "import scipy.linalg.blas as fblas\n",
    "ctypedef np.float32_t REAL_t\n",
    "cdef int ONE = 1\n",
    "\n",
    "\n",
    "REAL = np.float32\n",
    "cdef extern from \"/Users/mayk/working/figer/baseline/PLE/Model/warp/voidptr.h\":\n",
    "    void* PyCObject_AsVoidPtr(object obj)\n",
    "DEF MAX_SENTENCE_LEN = 10000\n",
    "ctypedef void (*scopy_ptr) (const int *N, const float *X, const int *incX, float *Y, const int *incY) nogil\n",
    "ctypedef void (*saxpy_ptr) (const int *N, const float *alpha, const float *X, const int *incX, float *Y, const int *incY) nogil\n",
    "ctypedef float (*sdot_ptr) (const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil\n",
    "ctypedef double (*dsdot_ptr) (const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil\n",
    "ctypedef double (*snrm2_ptr) (const int *N, const float *X, const int *incX) nogil\n",
    "ctypedef void (*sscal_ptr) (const int *N, const float *alpha, const float *X, const int *incX) nogil\n",
    "\n",
    "\n",
    "cdef scopy_ptr scopy = <scopy_ptr>PyCObject_AsVoidPtr(fblas.scopy._cpointer)  # y = x\n",
    "cdef saxpy_ptr saxpy=<saxpy_ptr>PyCObject_AsVoidPtr(fblas.saxpy._cpointer)  # y += alpha * x\n",
    "cdef sdot_ptr sdot=<sdot_ptr>PyCObject_AsVoidPtr(fblas.sdot._cpointer)  # float = dot(x, y)\n",
    "cdef dsdot_ptr dsdot=<dsdot_ptr>PyCObject_AsVoidPtr(fblas.sdot._cpointer)  # double = dot(x, y)\n",
    "cdef snrm2_ptr snrm2=<snrm2_ptr>PyCObject_AsVoidPtr(fblas.snrm2._cpointer)  # sqrt(x^2)\n",
    "cdef sscal_ptr sscal=<sscal_ptr>PyCObject_AsVoidPtr(fblas.sscal._cpointer) # x = alpha * x\n",
    "DEF EXP_TABLE_SIZE = 10000\n",
    "DEF MAX_EXP = 50\n",
    "\n",
    "cdef REAL_t[EXP_TABLE_SIZE] EXP_TABLE\n",
    "cdef REAL_t[EXP_TABLE_SIZE] LOG_TABLE\n",
    "\n",
    "cdef REAL_t ONEF = <REAL_t>1.0\n",
    "\n",
    "# for when fblas.sdot returns a double\n",
    "cdef REAL_t our_dot_double(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    return <REAL_t>dsdot(N, X, incX, Y, incY)\n",
    "\n",
    "# for when fblas.sdot returns a float\n",
    "cdef REAL_t our_dot_float(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    return <REAL_t>sdot(N, X, incX, Y, incY)\n",
    "\n",
    "# for when no blas available\n",
    "cdef REAL_t our_dot_noblas(const int *N, const float *X, const int *incX, const float *Y, const int *incY) nogil:\n",
    "    # not a true full dot()-implementation: just enough for our cases\n",
    "    cdef int i\n",
    "    cdef REAL_t a\n",
    "    a = <REAL_t>0.0\n",
    "    for i from 0 <= i < 50 by 1:\n",
    "        a += X[i] * Y[i]\n",
    "    return a\n",
    "\n",
    "# for when no blas available\n",
    "cdef void our_saxpy_noblas(const int *N, const float *alpha, const float *X, const int *incX, float *Y, const int *incY) nogil:\n",
    "    cdef int i\n",
    "    for i from 0 <= i < N[0] by 1:\n",
    "        Y[i * (incY[0])] = (alpha[0]) * X[i * (incX[0])] + Y[i * (incY[0])]\n",
    "cdef REAL_t cvdot(vec1,vec2,size):\n",
    "    cdef int csize = size\n",
    "    f= dsdot(&csize,<REAL_t *>(np.PyArray_DATA(vec1)),&ONE,<REAL_t *>(np.PyArray_DATA(vec2)),&ONE)\n",
    "    return f\n",
    "def csaxpy(vec1,vec2,alpha,size):\n",
    "    cdef int csize = size\n",
    "    cdef float calpha = alpha\n",
    "    f= our_saxpy_noblas(&csize,&calpha,<REAL_t *>(np.PyArray_DATA(vec1)),&ONE,<REAL_t *>(np.PyArray_DATA(vec2)),&ONE)\n",
    "    return f\n",
    "cdef REAL_t crank(int k):\n",
    "    cdef REAL_t loss = 0.\n",
    "    cdef int i = 1\n",
    "    for i in range(1,k+1):\n",
    "        loss += ONEF/i\n",
    "    return loss\n",
    "cdef REAL_t vsum(REAL_t *vec,int *size):\n",
    "    cdef int i\n",
    "    cdef REAL_t product\n",
    "    product = <REAL_t>0.0\n",
    "    for i from 0 <= i < size[0] by 1:\n",
    "        product += vec[i] **2\n",
    "    return sqrt(product)\n",
    "def cnorm(vec):\n",
    "    cdef int size\n",
    "    size  = len(vec)\n",
    "    return vsum(<REAL_t *>(np.PyArray_DATA(vec)),&size)\n",
    "def init():\n",
    "    for i in range(EXP_TABLE_SIZE):\n",
    "        EXP_TABLE[i] = <REAL_t>exp((i / <REAL_t>EXP_TABLE_SIZE * 2 - 1) * MAX_EXP)\n",
    "        EXP_TABLE[i] = <REAL_t>(EXP_TABLE[i] / (EXP_TABLE[i] + 1))\n",
    "#init()\n",
    "\n",
    "\n",
    "def ctrain(A,B,insts,size,lr,gradient,max_it =10):\n",
    "    cdef float error\n",
    "    next_random = 1\n",
    "    for it in range(1,max_it+1):\n",
    "        error = 0.\n",
    "        for i,inst in enumerate(insts):\n",
    "            err,next_random =gradient(A,B,inst,size,next_random,lr=lr)\n",
    "            error += err\n",
    "#             if i % 1000 ==0:\n",
    "#                 sys.stdout.write(\"\\rIteration %d \" % (it)+ \"trained {0:.0f}%\".format(float(i)*100/len(insts))+\" Loss:{0:.2f}\".format(error))\n",
    "#                 sys.stdout.flush()\n",
    "#         sys.stdout.write(\"\\n\")\n",
    "    return error\n",
    "\n",
    "cdef void divide(REAL_t *vec, const float *alpha, const int *size):\n",
    "    cdef int i\n",
    "    for i from 0 <= i < size[0] by 1:\n",
    "        vec[i] = vec[i]/alpha[0]\n",
    "def cdivide(vec,alpha):\n",
    "    cdef int size\n",
    "    size  = len(vec)\n",
    "    cdef float r = alpha\n",
    "    divide(<REAL_t *>(np.PyArray_DATA(vec)),&r,&size)\n",
    "\n",
    "    \n",
    "def max_margin_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros([len(inst.labels),size],dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0\n",
    "    cdef REAL_t s1,s2\n",
    "    cdef clr = lr\n",
    "    cdef int N\n",
    "    cdef int n_sample\n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= cvdot(x,B[l],50)\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in range(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            s2 = cvdot(x,B[nl],50)\n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "            error += 1 + s2-s1\n",
    "            csaxpy(B[l]-B[n_sample],dA,1.0,50)\n",
    "            csaxpy(x,dB[l],1.0,50)\n",
    "            csaxpy(x,dB[n_sample],-1.0,50)\n",
    "\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "\n",
    "    for i in range(len(B)):\n",
    "        csaxpy(dB[i],B[i],clr,50)\n",
    "        #B[i] += lr*dB[i]\n",
    "        norm =  cnorm(B[i])\n",
    "        if norm >1:\n",
    "            cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "\n",
    "def max_max_margin_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros([len(inst.labels),size],dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0\n",
    "    cdef REAL_t s1,s2\n",
    "    cdef clr = lr\n",
    "    cdef int N\n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef int max_l,max_nl\n",
    "    cdef REAL_t max_s1= float('-inf'),max_s2=float('-inf')\n",
    "    for l in inst.sparse_labels:\n",
    "        s1= cvdot(x,B[l],50)\n",
    "        if s1>=max_s1:\n",
    "            max_l = l\n",
    "            max_s1 = s1\n",
    "    for nl in inst.negative_labels:\n",
    "        s2= cvdot(x,B[nl],50)\n",
    "        if s2>=max_s2:\n",
    "            max_nl = nl\n",
    "            max_s2 = s2\n",
    "    if max_s1-max_s2<1:\n",
    "        error += 1 + max_s2-max_s1\n",
    "        csaxpy(B[max_l]-B[max_nl],dA,1.0,50)\n",
    "        csaxpy(x,dB[max_l],1.0,50)\n",
    "        csaxpy(x,dB[max_nl],-1.0,50)\n",
    "\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "\n",
    "    for i in range(len(B)):\n",
    "        csaxpy(dB[i],B[i],clr,50)\n",
    "        #B[i] += lr*dB[i]\n",
    "        norm =  cnorm(B[i])\n",
    "        if norm >1:\n",
    "            cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "def softmax_gradient(A,B,inst,size,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    random.seed(1)\n",
    "    x = np.mean(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0\n",
    "    cdef REAL_t s1,s2,logs1,logs2,g\n",
    "    cdef clr = lr\n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef REAL_t pos = ONEF\n",
    "    cdef int csize =size\n",
    "    for l in inst.sparse_labels:\n",
    "        logs1= cvdot(x,B[l],50)\n",
    "        if  logs1 <= -MAX_EXP or  logs1 >= MAX_EXP:\n",
    "            continue\n",
    "        s1 = EXP_TABLE[<int>((logs1 + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]\n",
    "        error -=logs1\n",
    "        g = (pos - s1) * clr\n",
    "        csaxpy(B[l],dA,g,50)\n",
    "        saxpy(&csize, &g, <REAL_t *>(np.PyArray_DATA(x)), &ONE, <REAL_t *>(np.PyArray_DATA(B[l])), &ONE)\n",
    "        for k in range(neg_num):\n",
    "            nl = inst.negative_labels[randint(0,neg_num-1)]\n",
    "            logs2 = cvdot(x,B[nl],50)\n",
    "            if  logs2 <= -MAX_EXP or  logs2 >= MAX_EXP:\n",
    "                continue\n",
    "            s2 = EXP_TABLE[<int>((logs2 + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]\n",
    "            if s2 > s1:\n",
    "                g =  (-s2) * clr\n",
    "                csaxpy(B[nl],dA,g,50)\n",
    "                saxpy(&csize, &g, <REAL_t *>(np.PyArray_DATA(x)), &ONE, <REAL_t *>(np.PyArray_DATA(B[nl])), &ONE)\n",
    "                error += logs2\n",
    "    \n",
    "\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        #if norm >1:\n",
    "         #   cdivide(A[f],norm)\n",
    "\n",
    "    for i in range(len(B)):\n",
    "        norm =  cnorm(B[i])\n",
    "       # if norm >1:\n",
    "        #    cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error\n",
    "\n",
    "def warp_gradient(A,B,inst,size,next_random,lr=0.01):\n",
    "    #print B\n",
    "    #print B[0]-B[9]\n",
    "    cdef unsigned long long  c_next_random = next_random\n",
    "    dA = np.zeros(size,dtype=REAL)\n",
    "    dB = np.zeros([len(inst.labels),size],dtype=REAL)\n",
    "    x = np.sum(A[inst.features],axis=0)\n",
    "    cdef REAL_t error = 0.\n",
    "    cdef REAL_t clr = lr\n",
    "    cdef int N,n_sample \n",
    "    cdef int neg_num = len(inst.negative_labels)\n",
    "    cdef REAL_t norm\n",
    "    cdef int cSize = size\n",
    "    cdef REAL_t floats\n",
    "    \n",
    "   \n",
    "    for l in inst.sparse_labels:\n",
    "        s1= cvdot(x,B[l],50)\n",
    "        N=1\n",
    "        n_sample  = -1\n",
    "        for k in range(neg_num):\n",
    "            c_next_random = random_int32(&c_next_random)\n",
    "            nl = inst.negative_labels[c_next_random%neg_num]#randint(0,neg_num-1)]\n",
    "            s2 = cvdot(x,B[nl],50)\n",
    "            \n",
    "            if s1 - s2<1:\n",
    "                n_sample = nl\n",
    "                N = k+1\n",
    "                break\n",
    "        if n_sample!=-1:\n",
    "\n",
    "            L = crank(len(inst.negative_labels)/N)\n",
    "            negL = -L\n",
    "            error += (1+s2-s1)*L\n",
    "\n",
    "            csaxpy(B[l]-B[n_sample],dA,L,50)\n",
    "            \n",
    "            csaxpy(x,dB[l],L,50)\n",
    "            csaxpy(x,dB[n_sample],-L,50)\n",
    "            #print dB[l][0]\n",
    "    for f in inst.features:\n",
    "        csaxpy(dA,A[f],clr,50)\n",
    "        norm = cnorm(A[f])\n",
    "        if norm >1:\n",
    "            cdivide(A[f],norm)\n",
    "\n",
    "    for i in range(len(B)):\n",
    "        csaxpy(dB[i],B[i],clr,50)\n",
    "\n",
    "        #B[i] += lr*dB[i]\n",
    "        norm =  cnorm(B[i])\n",
    "        if norm >1:\n",
    "            cdivide(B[i],norm)\n",
    "            #B[i] /=norm\n",
    "    return error,c_next_random\n",
    "def save_to_text(matrix,output):\n",
    "    shape = matrix.shape\n",
    "    with open(output,'wb') as out:\n",
    "        out.write(\"%d %d\\n\" % (shape))\n",
    "        for row in matrix:\n",
    "            x = \" \".join(map(lambda x:\"{0:.5}\".format(x),row))\n",
    "            out.write(x+\"\\n\")\n",
    "\n",
    "cdef inline unsigned long long random_int32(unsigned long long *next_random) nogil:\n",
    "    next_random[0] = (next_random[0] * <unsigned long long>25214903917ULL + 11) & 281474976710655ULL\n",
    "    return next_random[0]\n",
    "def crand(sed):\n",
    "    cdef unsigned long long csed = sed\n",
    "    return random_int32(&csed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "try:\n",
    "    from queue import Queue, Empty\n",
    "except ImportError:\n",
    "    from Queue import Queue, Empty\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "size=50\n",
    "np.random.seed(1)\n",
    "A= np.random.rand(len(a.feature2id),size).astype(np.float32)\n",
    "# for i in xrange(len(A)):\n",
    "#     norm =  cnorm(A[i])\n",
    "#     if norm >1:\n",
    "#         cdivide(A[i],norm)\n",
    "B= np.random.rand(len(a.label2id),size).astype(np.float32)\n",
    "insts = a.data\n",
    "def train(A,B,insts,it):\n",
    "    queue_factor = 3\n",
    "    works = 20\n",
    "    batch = 8000\n",
    "    def job_producer():\n",
    "        \"\"\"Fill jobs queue using the input `sentences` iterator.\"\"\"\n",
    "        job_batch, batch_size = [], 0\n",
    "        pushed_words, pushed_examples = 0, 0\n",
    "        job_no = 0\n",
    "\n",
    "        for inst_idx, inst in enumerate(insts):\n",
    "            # can we fit this sentence into the existing job batch?\n",
    "            if batch_size + 1 <= batch:\n",
    "                # yes => add it to the current job\n",
    "                job_batch.append(inst)\n",
    "                batch_size += 1\n",
    "            else:\n",
    "                # no => submit the existing job\n",
    "                logger.debug(\n",
    "                    \"queueing job #%i (%i words, %i sentences)\",\n",
    "                    job_no, batch_size, len(job_batch))\n",
    "                job_no += 1\n",
    "                job_queue.put((job_batch))\n",
    "                # add the sentence that didn't fit as the first item of a new job\n",
    "                job_batch, batch_size = [inst], 1\n",
    "        if job_batch:\n",
    "            logger.debug(\n",
    "                \"queueing job #%i (%i insts, %i insts)\",\n",
    "                job_no, batch_size, len(job_batch))\n",
    "            job_no += 1\n",
    "            job_queue.put((job_batch))\n",
    "        for _ in xrange(works):\n",
    "            job_queue.put(None)\n",
    "    def worker_loop():\n",
    "        jobs_processed = 0\n",
    "        while True:\n",
    "            job = job_queue.get()\n",
    "            if job is None:\n",
    "                progress_queue.put(None)\n",
    "                break  # no more jobs => quit this worker\n",
    "            insts = job\n",
    "            tally = ctrain(A,B,insts,50,0.01,warp_gradient,max_it=1)\n",
    "            progress_queue.put((len(insts), tally))  # report back progress\n",
    "            jobs_processed += 1\n",
    "    job_queue = Queue(maxsize=queue_factor *works )\n",
    "    progress_queue = Queue(maxsize=(queue_factor + 1) * works)\n",
    "    workers = [threading.Thread(target=worker_loop) for _ in xrange(works)]\n",
    "    unfinished_worker_count = len(workers)\n",
    "    workers.append(threading.Thread(target=job_producer))\n",
    "    for thread in workers:\n",
    "        thread.daemon = True  # make interrupting the process with ctrl+c easier\n",
    "        thread.start()\n",
    "    err =0\n",
    "    job_tally=0\n",
    "    while unfinished_worker_count > 0:\n",
    "        report = progress_queue.get()  # blocks if workers too slow\n",
    "        if report != None:\n",
    "            err += report[1]\n",
    "            sys.stdout.write(\"\\rIteration %d \" % (it)+ \" Loss:{0:.2f}\".format(err))\n",
    "        if report is None:  # a thread reporting that it finished\n",
    "            unfinished_worker_count -= 1\n",
    "            logger.info(\"worker thread finished; awaiting finish of %i more threads\", unfinished_worker_count)\n",
    "            continue\n",
    "        job_tally += 1\n",
    "        sys.stdout.flush()\n",
    "    sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time for i in range(10):train(A,B,insts,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
